data:
  train_file: "sample-10BT"
  batch_size: 32
  num_workers: 4
  tokenizer_path: "roberta-base"
  limit: 100
  min_length: 100
  min_sentences: 2

model:
  max_length: 512
  pred_dim: 384
  embed_dim: 768
  num_layers: 12
  num_heads: 12
  mlp_ratio: 4.0
  dropout: 0.1

optimization:
  epochs: 5
  lr: 0.001
  warmup: 1
  weight_decay: 0.04
  final_weight_decay: 0.4
  final_lr: 0.000001

logging:
  log_dir: "logs/lang_jepa_exp1"
  log_freq: 50
  checkpoint_freq: 1

meta:
  use_bfloat16: false
  load_checkpoint: false
  checkpoint_path: null